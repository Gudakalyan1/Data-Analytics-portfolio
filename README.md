# project: 1

# credit card default propensity prediction

https://github.com/Gudakalyan1/Kalyan-G.Github.io./blob/main/Credit%20card%20default%20propensity%20prediction.ipynb

Predict the probability of a customer defaulting payment for the credit card the subsequent month, based on past information. The past information is provided in the dataset. This probability will help the collections team to prioritise follow up with customers who have a high propensity of defaulting.
DecissionTree model achieve the  best accuracy out of all classification algorithams

classification report test: 
               precision    recall  f1-score   support

           0       0.86      0.91      0.88      7060
           1       0.58      0.44      0.50      1940

    accuracy                           0.81      9000
   macro avg       0.72      0.68      0.69      9000
weighted avg       0.80      0.81      0.80      9000



# project:2 
# Diabetic prediction

https://github.com/Gudakalyan1/Kalyan-G.Github.io./blob/main/_Pima%20indian%20diabetes.ipynb
 
 In this project i build a machine learning model using k nearest neighbor algoritham to predict whether the patiens in pima indian dataset have dabetics or not
  the knn classifier with number of neighbors as 14 achieve the best score of 75%.
 
 # project:3
  # car safety 
  
  https://github.com/Gudakalyan1/Kalyan-G.Github.io./blob/main/car%20safety.ipynb
 
   I built a decission tree classifier to predict the safety of the car.I build two models, one with criterion gini index and another one with criterion entropy. I implement       decision Tree Classification with Python and Scikit-Learn.
   The model yeilds a very good performance as indicated by the model accuracy in both cases which was found to be 89% on test data set.
   Gini index- train set=93%,test set=89%
   similarly the model criteration as entropy the train set accuracy is 93% and test accuracy 91% 
   
                    precision    recall  f1-score   support

         acc       0.94      0.76      0.84       116
        good       0.50      0.52      0.51        21
       unacc       0.96      0.99      0.97       358
       vgood       0.71      1.00      0.83        24

    accuracy                           0.92       519
   macro avg       0.78      0.82      0.79       519
weighted avg       0.92      0.92      0.92       519
   
